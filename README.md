
# Pesquisa Academica RAG

RAG (Retrieval-Augmented Generation) para **busca, leitura e sumariza√ß√£o de artigos acad√™micos em PT-BR**, com pipeline de **ingest√£o**, **indexa√ß√£o FAISS**, **embeddings Sentence-Transformers** e **API FastAPI**. Suporta LLM local (Transformers/Ollama) ou servi√ßos (OpenAI/HF Inference).

> **Objetivo**: permitir que qualquer pessoa carregue PDFs/textos acad√™micos e fa√ßa perguntas naturais, recebendo respostas citadas e passagens de suporte.

---

## ‚ú® Funcionalidades

* **Ingest√£o** de PDFs/TXT/MD/HTML com chunking configur√°vel
* **Embeddings** multil√≠ngues (recomendado: `intfloat/multilingual-e5-base`)
* **Indexa√ß√£o vetorial** com **FAISS** (CPU por padr√£o)
* **Busca h√≠brida** (BM25 opcional) + Sem√¢ntica (kNN / MMR)
* **RAG**: recupera√ß√£o de trechos + gera√ß√£o com LLM
* **API REST** (FastAPI) com **/docs (Swagger)**
* **CLI** para ingest√£o e queries locais
* **Docker** pronto (CPU)
* **Cita√ß√µes**: trechos e metadados da fonte retornados junto √† resposta
* **Configura√ß√£o via `.env`** (modelos, caminhos, top\_k, temperatura‚Ä¶)

---

## üèóÔ∏è Arquitetura & Fluxo

```
                +--------------------+
                |   Usu√°rio/Cliente  |
                |  (cURL/Swagger/CLI)|
                +----------+---------+
                           |
                           v
                     FastAPI /query
                           |
                           v
   +---------------- Recuperador ----------------+
   |  FAISS Index  |  (BM25 opc.) |  Filtros     |
   |  embeddings   |  (rank/merge)|  metadados   |
   +--------+------+--------------+--------------+
            | top_k
            v
       Passagens     +------------------------------+
   +---------------->|   Gerador (LLM)              |
   |                 |  (Transformers/Ollama/OpenAI)|
   |                 +------------------------------+
   |                              |
   |<------------- Resposta + cita√ß√µes -------------+
```

**Pastas sugeridas**

```
app/
  main.py               # API FastAPI
  rag/
    pipeline.py         # Orquestra RAG
    retriever.py        # Busca FAISS (+ BM25 opcional)
    embedder.py         # Carrega modelo de embeddings
    generator.py        # Abstrai backends LLM
    utils.py            # Parse, chunking, etc.
configs/
  settings.py           # Pydantic/BaseSettings (l√™ .env)
data/
  docs/                 # Documentos brutos (n√£o versionado)
  index/                # √çndices FAISS persistidos
scripts/
  ingest.py             # CLI: ingest√£o e indexa√ß√£o
  query.py              # CLI: pergunta r√°pida via terminal
tests/
  ...
```

---

## ‚úÖ Requisitos

* **Python 3.11+**
* **pip/venv** (ou conda/poetry)
* **CPU** basta; **GPU** opcional (PyTorch com CUDA)
* **faiss-cpu** (ou `faiss-gpu` se desejar)
* Acesso √† internet na primeira execu√ß√£o para baixar modelos (se usar Transformers) ‚Äî ou use **Ollama**/servi√ßo externo.

---

## ‚öôÔ∏è Instala√ß√£o

```bash
# 1) Clonar o repo
git clone https://github.com/SEU-USUARIO/pesquisa-academica-rag.git
cd pesquisa-academica-rag

# 2) Ambiente virtual
python -m venv .venv
# Linux/macOS:
source .venv/bin/activate
# Windows:
# .venv\Scripts\activate

# 3) Depend√™ncias
pip install --upgrade pip
pip install -r requirements.txt
# Obs.: em alguns ambientes Linux √© necess√°rio:
# pip install faiss-cpu --no-cache-dir
```

### `requirements.txt` (sugest√£o)

```txt
fastapi>=0.111
uvicorn[standard]>=0.30
python-dotenv>=1.0
pydantic>=2.7
sentence-transformers>=2.7
faiss-cpu>=1.8
transformers>=4.43
accelerate>=0.33
# Instale o torch adequado ao seu SO/GPU:
torch>=2.3
pypdf>=4.2
tqdm>=4.66
httpx>=0.27
rich>=13.7
typer>=0.12
orjson>=3.10
```

> **Torch**: caso tenha GPU NVIDIA, instale a vers√£o com CUDA (veja instru√ß√µes no site do PyTorch).

---

## üîß Configura√ß√£o

Crie um `.env` na raiz (baseado no exemplo abaixo):

```env
# ===== Embeddings =====
EMBEDDINGS_MODEL=intfloat/multilingual-e5-base

# ===== Indexa√ß√£o =====
DOCS_DIR=./data/docs
INDEX_DIR=./data/index
CHUNK_SIZE=512
CHUNK_OVERLAP=64
USE_BM25=false

# ===== Recupera√ß√£o =====
TOP_K=5
MMR=false
MMR_LAMBDA=0.5

# ===== LLM (escolha um backend) =====
# transformers | openai | ollama | hf_inference
LLM_BACKEND=transformers

# Transformers (local)
LLM_MODEL=google/gemma-2-2b-it
MAX_TOKENS=512
TEMPERATURE=0.2

# OpenAI
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:8b-instruct

# HF Inference API
HF_API_KEY=
HF_INFERENCE_MODEL=mistralai/Mistral-7B-Instruct-v0.2

# ===== API =====
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=info
```

> **Dicas de modelo**
>
> * **Embeddings**: `intfloat/multilingual-e5-base` (excelente para PT-BR).
> * **LLM leve (CPU)**: `google/gemma-2-2b-it` (Transformers) ou `llama3:8b-instruct` via Ollama.
> * **Servi√ßo**: `gpt-4o-mini` (OpenAI) ou `Mistral-7B-Instruct` (HF Inference).

---

## üßæ Ingest√£o de Documentos

Coloque seus arquivos em `./data/docs` (ou passe `--path`).

Formatos suportados inicialmente: **.pdf, .txt, .md, .html**

```bash
# ingest√£o + indexa√ß√£o
python scripts/ingest.py --path ./data/docs \
  --chunk-size 512 --chunk-overlap 64 \
  --recreate
```

**Par√¢metros comuns**

* `--path`: pasta com documentos
* `--recreate`: recria o √≠ndice do zero
* `--append`: adiciona ao √≠ndice existente
* `--chunk-size`, `--chunk-overlap`: controle de segmenta√ß√£o
* `--pattern "*.pdf"`: filtra extens√£o

> Ap√≥s a ingest√£o, o √≠ndice FAISS √© salvo em `./data/index`.

---

## üöÄ Executando a API

```bash
# com .env configurado
uvicorn app.main:app --host ${API_HOST:-0.0.0.0} --port ${API_PORT:-8000} --reload
# Abra: http://localhost:8000/docs
```

### Endpoints principais

* `GET /health` ‚Äì sa√∫de do servi√ßo
* `POST /query` ‚Äì pergunta com RAG
* `POST /rerank` (opcional) ‚Äì reordena passagens recuperadas
* `POST /ingest` (opcional) ‚Äì ingest√£o via API (para automa√ß√µes)

#### `POST /query` ‚Äì Exemplo de payload

```json
{
  "question": "Quais s√£o os principais m√©todos para detec√ß√£o de t√≥picos em portugu√™s?",
  "top_k": 5,
  "mmr": false,
  "filters": {
    "source": ["meuartigo.pdf"]
  }
}
```

#### Resposta (exemplo)

```json
{
  "answer": "Os m√©todos mais comuns incluem LDA, NMF e BERTopic...",
  "citations": [
    {
      "doc_id": "meuartigo.pdf",
      "page": 7,
      "score": 0.82,
      "text": "O BERTopic combina embeddings e clustering HDBSCAN..."
    }
  ],
  "used_model": "google/gemma-2-2b-it",
  "top_k": 5,
  "timing_ms": 842
}
```

---

## üß™ Exemplos de uso

### cURL

```bash
curl -s http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question":"Resuma as contribui√ß√µes do artigo X","top_k":5}' | jq
```

### Python

```python
import httpx, json
payload = {"question": "Quais datasets foram usados no estudo Y?", "top_k": 5}
r = httpx.post("http://localhost:8000/query", json=payload, timeout=120)
print(json.dumps(r.json(), ensure_ascii=False, indent=2))
```

### CLI r√°pida (sem API)

```bash
python scripts/query.py --q "Explique o m√©todo proposto no paper Z" --top-k 5
```

---

## üê≥ Docker

### Build & Run (CPU)

```bash
docker build -t pesquisa-academica-rag .
docker run --rm -it -p 8000:8000 \
  --env-file .env \
  -v $PWD/data:/app/data \
  pesquisa-academica-rag
# http://localhost:8000/docs
```

> O volume `-v $PWD/data:/app/data` garante persist√™ncia do √≠ndice FAISS.

### Compose (opcional)

```yaml
# docker-compose.yml
services:
  rag:
    build: .
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - ./data:/app/data
```

```bash
docker compose up --build
```

---

## ‚ö° Performance & Qualidade

* **Embeddings**: E5 √© robusto em PT-BR; para mais precis√£o tente `multilingual-e5-large` (custa mais RAM).
* **Chunking**: comece com `512/64`. Conte√∫do muito t√©cnico pode melhorar com `800-1000` tokens.
* **top\_k**: 5‚Äì8 √© um bom range; ative **MMR** em cole√ß√µes redundantes.
* **Reranking**: adicione um reranker (ex.: `cross-encoder/ms-marco-MiniLM-L-6-v2`) se a precis√£o de recupera√ß√£o for cr√≠tica.
* **Cache**: ative cache de embeddings/gera√ß√£o (ex.: disco) para acelerar re-execu√ß√µes.
* **GPU**: se dispon√≠vel, habilite `device_map="auto"` no `generator.py`/`embedder.py`.

---

## ü©∫ Troubleshooting

* **FAISS n√£o instala**: use `faiss-cpu` (ou `conda install -c pytorch faiss-cpu`).
* **Torch (GPU)**: instale a variante correta (CUDA 11/12) no site do PyTorch.
* **Mem√≥ria insuficiente**: use um LLM menor (`gemma-2-2b-it`/Ollama) ou reduza `MAX_TOKENS`.
* **PDFs com texto ‚Äúcolado‚Äù**: alguns PDFs t√™m OCR ruim; use uma etapa de OCR (Tesseract) antes de ingerir.
* **Resultados ruins**: aumente `CHUNK_SIZE`, ajuste `TOP_K`, ative `MMR`, e revise a limpeza dos textos.

---

## üîí Seguran√ßa & √âtica

* N√£o suba PDFs propriet√°rios sem permiss√£o.
* Respeite direitos autorais e **cite fontes**.
* O modelo pode alucinar; use as **cita√ß√µes** para verificar passagens.

---

## üß≠ Roadmap (sugest√£o)

* [ ] UI web simples (chat + upload)
* [ ] Reranking com Cross-Encoder
* [ ] Suporte nativo a **OCRTesseract** na ingest√£o
* [ ] Exportar respostas + cita√ß√µes em **BibTeX**
* [ ] Avalia√ß√£o RAG autom√°tica (Precision\@k, MRR, F1)
* [ ] Multi-√≠ndice + filtros avan√ßados por metadados

---

## üßπ Scripts √∫teis

```bash
# Recriar √≠ndice do zero
python scripts/ingest.py --path ./data/docs --recreate

# Adicionar novos docs
python scripts/ingest.py --path ./data/docs --append

# Perguntar por CLI
python scripts/query.py --q "Qual √© a contribui√ß√£o central do artigo A?"
```

---

## üß™ Testes

* Testes unit√°rios de utilit√°rios e do retriever (mocks) em `tests/`.
* Sugest√£o (pytest):

```bash
pip install pytest
pytest -q
```

---

## üìÑ Licen√ßa

**MIT** ‚Äî sinta-se livre para usar e melhorar. Inclua as cita√ß√µes adequadas ao compartilhar resultados.

---

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas!

1. Fa√ßa um fork
2. Crie uma branch: `feat/minha-ideia`
3. Commit: `feat: descri√ß√£o`
4. PR com descri√ß√£o clara (screenshots e exemplos ajudam!)

---

## üôè Agradecimentos

* [Sentence-Transformers](https://www.sbert.net/)
* [FAISS](https://faiss.ai/)
* [Transformers](https://huggingface.co/docs/transformers/)
* Comunidade RAG/open-source üöÄ

---

## üìé Anexos (exemplos r√°pidos de c√≥digo)

### `scripts/ingest.py` (esqueleto m√≠nimo)

```python
# scripts/ingest.py
import argparse, os
from app.rag.embedder import Embeddings
from app.rag.retriever import FaissIndex
from app.rag.utils import load_and_chunk_all

def main():
    p = argparse.ArgumentParser()
    p.add_argument("--path", required=True)
    p.add_argument("--chunk-size", type=int, default=int(os.getenv("CHUNK_SIZE", 512)))
    p.add_argument("--chunk-overlap", type=int, default=int(os.getenv("CHUNK_OVERLAP", 64)))
    m = p.add_mutually_exclusive_group()
    m.add_argument("--recreate", action="store_true")
    m.add_argument("--append", action="store_true")
    args = p.parse_args()

    texts, metas = load_and_chunk_all(args.path, args.chunk_size, args.chunk_overlap)
    emb = Embeddings()
    idx = FaissIndex(recreate=args.recreate)
    idx.add(texts, metas, emb)
    idx.save()
    print(f"Index pronto em {os.getenv('INDEX_DIR','./data/index')}")

if __name__ == "__main__":
    main()
```

### `scripts/query.py` (esqueleto m√≠nimo)

```python
# scripts/query.py
import argparse
from app.rag.pipeline import RagPipeline

p = argparse.ArgumentParser()
p.add_argument("--q", required=True)
p.add_argument("--top-k", type=int, default=5)
args = p.parse_args()

rag = RagPipeline()
out = rag.answer(args.q, top_k=args.top_k)
print(out["answer"])
for c in out["citations"]:
    print(f"- {c['doc_id']} (score={c['score']:.2f}): {c['text'][:120]}...")
```

